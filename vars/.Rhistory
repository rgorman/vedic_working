sum(v*v)^.5
3.605551 + 0.7071068
.5/ (3.605551 + 0.7071068)
.5/ (3.605551 + 0.7071068) %>%
acos()
.5/ (3.605551 + 0.7071068) %>%
cos()
acos(-1.284783)
sum(u*u)
(u*v)
(u*v) %>%
sum()
sum(u*u)^.5
sum(v*v)^.5
sum(v*v)
u*v
u <- c(-2, 3)
v <- c(1/2, -1/2)
(u*v) %>%
sum()
sum(u*u)^.5
sum(v*v)^.5
sum(u*v)
sum(u*v)/ 9(sum(u*u)^.5) * (sum(v*v)^.5) )
sum(u*v)/ ( (sum(u*u)^.5) * (sum(v*v)^.5) )
sum(u*v)
cos(-2.5)
acos(0.8011436)
acos(-0.8011436)
install.packages("xlsx")
install.packages("writexl")
require(tidyverse)
require(xml2)
install.packages("ndl")
library(ndl)
RescorlaWagner
data(lexample)
lexample$Cues <- orthoCoding(lexample$Word, grams=1)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand")
lexample$Cues
data(lexample)
lexample
lexample$Cues <- orthoCoding(lexample$Word, grams=1)
lexample
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand")
lexample.rw
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="and")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="n", traceOutcome="and")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="n", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="nd", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="nd", traceOutcome="hand")
lexample
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="n", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="l", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="a", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hands")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="plural_hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand_plural")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="dog")
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="lass")
plot(lexample.rw)
lexample
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome=c("hand"))
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome=c("hand", "plural"))
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="h", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="d", traceOutcome="hand")
plot(lexample.rw)
cueCoding(cues = c("hello", "world"), maxn=1, adjacent=FALSE)
cueCoding("hand")
cueCoding("hand", maxn = 1)
cues <- c("First", "Plural", "hs95")
cueCoding(cues, maxn=1)
cueCoding(cues, maxn=2)
orthoCoding("hand")
orthoCoding("hand", grams = 1)
orthoCoding("hand", grams = 2)
lexample$Cues <- orthoCoding(lexample$Word, grams=2)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="d", traceOutcome="hand")
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="nd", traceOutcome="hand")
plot(lexample.rw)
lexample
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="ha", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="an", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="la", traceOutcome="hand")
plot(lexample.rw)
lexample.rw <- RescorlaWagner(lexample, nruns=25,
traceCue="la", traceOutcome="land")
plot(lexample.rw)
x <- vector(mode = "integer", 10)
names(x) <- c("casnr", "arch", "asc", "bus", "ehs", "eng", "fpa", "jmc", "pac", "chan")
x
x <- c(115, 2, 165, 180, 134, 82, 22, 60, 29, 10)
x
sum(x)
x <- c(115, 2, 165, 180, 134, 82, 22, 60, 29, 13)
sum(x)
sum(x)[1:9]
sum(x[1:9])
sum(x[1:10])
x <- c(112, 2, 164, 179, 128, 82, 22, 58, 29, 13)
sum(x[1:10])
choose(x, 5)
choose(5, x)
choose(1, 5)
choose(1, 5)
choose(5, 2)
choose(5, 1)
combn(5, 1,)
combn(5, 2,)
combn(x, 5)
combn(x, 2)
choose(10, 5)
combn(x, 2, simplify = T)
y <- combn(x, 2, simplify = T)
y <- combn(x, 5, simplify = T)
t <- 789/2
s <- colSums(y)
s
s - t
min(s - t)
min(abs(s - t))
m <- min(abs(s - t))
which(m == 1.5)
which(abs(s - t) == m)
y[84, ]
y[, 84 ]
x
names(x) <- c("casnr", "arch", "asc", "bus", "ehs", "eng", "fpa", "jmc", "pac", "chan")
x
y[,116]
y[,137]
y[, 169]
197*2
choose(10, 4)
y <- combn(x, 4, simplify = T)
s <- colSums(y)
m <- min(abs(s - t))
which(abs(s - t) == m)
y[, 55]
z <- c(2, 3, 5, 8, 9, 10)
x[z]
sum(x[z])
sum(x[z])/2
c1 <- combn(x[z], 3)
c1
c1 <- combn(x[z], 3, simplify = T)
c1 <- combn(x[z], 3, simplify = F)
c1
sum(c1)
sapply(c1, sum)
s <- sapply(c1, sum)
abs(s - 197)
c1[3]
seq(1, 10)
seq(3002, length.out  = 22)
seq(3002, length.out  = 22, by = 2)
seq(8002, length.out  = 22, by = 2)
seq(8002, length.out  = 29, by = 2)
seq(201, length.out  = 29, by = 1)
seq(201, length.out  = 128, by = 1)
.8/.057
require(tidyverse)
a <- c("POS". "G", "N", "C", "Dep")
a <- c("POS", "G", "N", "C", "Dep")
b c("POS", "G", "N", "C", "Dep")
b <-  c("POS", "G", "N", "C", "Dep")
combn(a)
combn(a, 1)
m <- 1:5
combn(a, )
combn(a, m)
combn(a, 1)
combn(a, 2)
combn(a, 3)
combn(a, 4)
combn(a, 5)
2^5
b <-  c("P_POS", "P_G", "P_N", "P_C", "P_Dep")
a <- c("S_POS", "S_G", "S_N", "S_C", "S_Dep")
combn(a, 5)
combn(c(a, b), 5)
combn(c(a, b), 1)
combn(c(a, b), 2)
combn(c(a, b), 3)
2^10
log(66)
x <- .833
exp(-x) / (1 + (exp(-x)))
exp(x) / (1 + (exp(x)))
## cross-entropy loss
y <- 1
y_hat <- 1
y_hat^y * ((1-y_hat)^(1-y))
y_hat <- .75
y_hat^y * ((1-y_hat)^(1-y))
y_hat^y
((1-y_hat)^(1-y))
(1-y_hat)
(1-y)
((1-y_hat)^(1-y))
y_hat^y * ((1-y_hat)^(1-y))
## cross-entropy loss
y <- 0
y_hat <- .75
y_hat^y * ((1-y_hat)^(1-y))
log(y_hat^y * ((1-y_hat)^(1-y)))
## cross-entropy loss
y <- 1
y_hat <- .75
y_hat^y * ((1-y_hat)^(1-y))
log(y_hat^y * ((1-y_hat)^(1-y)))
-log(y_hat^y * ((1-y_hat)^(1-y)))
y_hat <- .7
y_hat^y * ((1-y_hat)^(1-y))
-log(y_hat^y * ((1-y_hat)^(1-y)))
(y*log(y_hat)) + ((1-y)*log(1-y_hat))
-(y*log(y_hat)) + ((1-y)*log(1-y_hat))
## cross-entropy loss
y <- 0
y_hat^y * ((1-y_hat)^(1-y))
-log(y_hat^y * ((1-y_hat)^(1-y)))
-(y*log(y_hat)) + ((1-y)*log(1-y_hat))
(y*log(y_hat)) + ((1-y)*log(1-y_hat))
-(y*log(y_hat)) + ((1-y)*log(1-y_hat))
(-(y*log(y_hat)) + ((1-y)*log(1-y_hat)))
((y*log(y_hat)) + ((1-y)*log(1-y_hat)))
-1*(y*log(y_hat)) + ((1-y)*log(1-y_hat))
-log(y_hat^y * ((1-y_hat)^(1-y)))
(y*log(y_hat)) + ((1-y)*log(1-y_hat)) * -1
###
xx <- c(3, 2)
###
x <- c(3, 2)
exp(-x) / (1 + (exp(-x)))
w <- c(0, 0, 0)
x*w
###
x <- c(3, 2, 1)
w <- c(0, 0, 0)
x*w
exp(x*w) / (1 + (exp(x*w)))
ss <- exp(x*w) / (1 + (exp(x*w)))
ss - 1
(ss - 1) * x
g1 <- (ss - 1) * x
-.1 * g1
GNT.df <- readRDS("~/Github/GNT-stylometry/proiel/data/GNT_binary_valued.RDS")
colnames(GNT.df)[1:100]
require(tidyverse)
GNT.df$book_id %>% unique()
Hebr.df <- GNT.df %>%
filter(book_id = "HEB")
Hebr.df <- GNT.df %>%
filter(book_id == "HEB")
Rom.df <- GNT.df %>%
filter(book_id == "ROM")
GNT.df$self_proiel_pos %>% unique()
Hebr.df %>%
filter(self_proiel_pos == "common_noun")
View(Hebr.df)
Hebr.df %>%
filter(self_proiel_pos == "common_noun") %>%
select(id, form)
GNT.df$self_gender %>% unique()
GNT.df$self_gender
GNT.df$self_gender %>% unique()
Hebr.df %>%
filter(self_gender == "masculine_or_neuter")
Hebr.df %>%
filter(self_gender == "masculine_or_neuter") %>%
select(id, form)
View(Hebr.df)
Hebr.df %>%
filter(self_gender == "masculine_or_neuter") %>%
select(id, form, citation_part)
Hebr.df %>%
filter(self_case == "dative") %>%
select(id, form, citation_part)
Rom.df %>%
filter(self_case == "dative") %>%
select(id, form, citation_part)
Rom.df %>%
filter(self_case == "dative")
Rom.df %>%
filter(self_case == "dative") %>%
summarise(ct = n())
Rom.df %>%
filter(self_case == "dative") %>%
group_by(self_gender)
Rom.df %>%
filter(self_case == "dative") %>%
group_by(self_gender) %>%
summarise(ct = n())
x <- 20:40
20/x
30/33
names(x) <- x
20/x
25/40
25*(1/.625)
seq(1142, )
seq(1142, length.out = 141 )
seq(1283, length.out = 141 )
seq(1424, length.out = 141 )
seq(2001, length.out = 83 )
seq(2084, length.out = 83 )
seq(5223, length.out = 111 )
seq(5139, length.out = 138 )
seq(5277, length.out = 138 )
5277+138
140*20
seq(1103, length.out = 102 )
seq(1103, length.out = 100 )
seq(2085, length.out = 80 )
seq(2165, length.out = 80 )
seq(2245, length.out = 80 )
seq(3088, length.out = 90 )
seq(3178, length.out = 89 )
seq(5001, length.out = 135 )
seq(5136, length.out = 140 )
seq(5276, length.out = 140 )
seq(5416, length.out = 140 )
seq(5416, length.out = 135 )
seq(6001, length.out = 135 )
seq(6136, length.out = 140 )
seq(6276, length.out = 140 )
seq(6416, length.out = 140 )
seq(8084, length.out = 80 )
seq(1001, length.out = 101 )
seq(1102, length.out = 101 )
seq(1102, length.out = 110 )
seq(1212, length.out = 110 )
seq(1322, length.out = 110 )
seq(1322, length.out = 107 )
seq(2001, length.out = 82 )
seq(2082, length.out = 88 )
seq(2170, length.out = 88 )
seq(2258, length.out = 87 )
seq(2258, length.out = 22 )
seq(2001, length.out = 82 )
seq(3001, length.out = 90 )
seq(3091, length.out = 88 )
seq(3179, length.out = 88 )
seq(5144, length.out = 132 )
seq(5276, length.out = 132 )
seq(5408, length.out = 143 )
require(tidyverse)
x <- 1:5
combn(x, 2)
combn(x, 23)
combn(x, 5)
combn(x, 1)
combn(1:5, 1)
choose(5, 23)
setwd("~/Github/vedic/vedic_sandbox/data/renumbered")
files.v <- dir(pattern = ".RDS")
holder.l <- vector(mode = "list", length(files.v))
for (i in seq_along(files.v)) {
x <- readRDS(files.v[i])
holder.l[[i]] <- x
print(paste0("finished ", i))
}
require(tidyverse)
working.df <- do.call(bind_rows, holder.l)
nrow(working.df)
working.df$sentence_id
working.df$sentence_id %>%
unique() %>%
length()
working.df$sentence_id %>%
unique() %>%
length()
## add global token id
a <- 1:nrow(working.df)
working.df <- add_column(working.df, GlobalTokenId = a, .before = TRUE)
View(working.df)
sent.v <- working.df$sentence_id %>%
unique()
n <- 1
a <- working.df %>%
filter(sentence_id == sent.v[n]) # df with rows sentence by sentence
b <- as.numeric(a$head_token_id) # vector with head_token_id for each row in sentence
b[which(b == 0)] <- NA # eliminate any head_token_id with value 0
c(parent_holder.v,  a$term_id[b] %>%
as.numeric() )
a$GlobalTokenId[b]
a$GlobalTokenId[b]
parent_holder.v <- vector(mode = "integer", length(nrow(working.df)))
parent_holder.v <- vector(mode = "numeric", length(nrow(working.df)))
parent_holder.v <- vector(mode = "integer", length(nrow(working.df)))
parent_holder.v <- c(parent_holder.v,  a$GlobalTokenId[b] %>%
as.numeric() ) # add parent term_token_id values for current sentence to vector
b
parent_holder.v <- NULL
parent_holder.v <- c(parent_holder.v,  a$GlobalTokenId[b] %>%
as.numeric() ) # add parent term_token_id values for current sentence to vector
a$GlobalTokenId[b]
b <- as.numeric(a$head_token_id) # vector with head_token_id for each row in sentence
n <- 2
a <- working.df %>%
filter(sentence_id == sent.v[n]) # df with rows sentence by sentence
b <- as.numeric(a$head_token_id) # vector with head_token_id for each row in sentence
b[which(b == 0)] <- NA # eliminate any head_token_id with value 0
a$GlobalTokenId[b]
b
a$GlobalTokenId[b]
View(a)
parent_holder.v <- NULL
for (n in seq_along(sent.v)) { # loop to create vector of parent term_ids
a <- working.df %>%
filter(sentence_id == sent.v[n]) # df with rows sentence by sentence
b <- as.numeric(a$head_token_id) # vector with head_token_id for each row in sentence
b[which(b == 0)] <- NA # eliminate any head_token_id with value 0
parent_holder.v <- c(parent_holder.v,  a$GlobalTokenId[b] %>%
as.numeric() ) # add parent term_token_id values for current sentence to vector
}
parent_holder.v
working.df[180:200, 1:10]
working.df$head_token_id
working.df[180:200, 1:3]
working.df[180:200, c(1:3, 8, 14) ]
RV_1_3 <- readRDS("~/Github/vedic/vedic_sandbox/data/RV_1_3.RDS")
View(working.df)
View(RV_1_3)
setwd("~/Github/vedic/Hellwig/sanskrit/dcs/data/conllu/files/Ṛgveda")
require(udpipe)
files.v <- dir(pattern = "_parsed")
files.v
z <- udpipe_read_conllu(files.v[1])
View(z)
rm(list = ls())
files.v <- dir(pattern = "_parsed")
i <- 1
working.df <- udpipe_read_conllu(files.v[i])
z <- working.df$feats %>%
str_split(., "\\|", simplify = TRUE) # make a matrix of each name-value pair in the "feats" col of the input (e.g., "Case=Nom")
z %>% gsub("=.*", "", .)
z %>% gsub("=.*", "", .) %>%
unlist()
z %>% gsub("=.*", "", .) %>%
unlist() %>%
unique()
z %>% gsub("=.*", "", .) %>%
as.vector() %>%
unique()
holder.l <- vector(mode = "list", length(files.v))
for (i in seq_along(files.v)) {
working.df <- udpipe_read_conllu(files.v[i])
z <- working.df$feats %>%
str_split(., "\\|", simplify = TRUE) # make a matrix of each name-value pair in the "feats" col of the input (e.g., "Case=Nom")
holder.l[[i]] <-  z %>% gsub("=.*", "", .) %>%
as.vector() %>%
unique()
}
unlist(holder.l) %>% unique
morphs.v <-  unlist(holder.l) %>% unique
morphs.v <- morphs.v[- which(morphs.v == "")] # remove empty categories
morphs.v[which(is.na(morphs.v))] <- "Not_App" # rename NA: this is not an acceptable name for a col
BookAndHymn.v <- files.v %>% str_extract("ṚV.*") %>%
gsub("-.*", "", .) %>%
gsub(", ", "_", .) %>%
gsub("Ṛ", "R", .)
Book.v <- BookAndHymn.v %>%
gsub("RV_", "", .) %>%
gsub("_.*", "", .)
setwd("~/Github/vedic/vedic_sandbox/vars")
saveRDS(morphs.v, "morphs.RDS")
